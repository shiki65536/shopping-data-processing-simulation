{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Producing the data\n",
    "In this task, we will implement one Apache Kafka producer to simulate real-time data streaming. Spark is not allowed in this part since it’s simulating a streaming data source.\n",
    "\n",
    "1.1 Your program should send one batch of click_stream data every 5 seconds. One batch consists of a random 500-1000 rows from the clickstream_rt dataset. The CSV shouldn’t be loaded to memory at once to conserve memory (i.e. Read row as needed).  \n",
    "1.2 For each row, add an integer column named ‘ts’, a Unix timestamp in seconds since the epoch (UTC timezone). Spead your batch out evenly for 5 seconds.  \n",
    "For example, if you send a batch of 600 records at 2023-09-01 00:00:00 (ISO format: YYYY-MM-DD HH:MM:SS) -> (ts = 1693526400) :  \n",
    "Record 1-120: ts = 1693526400  \n",
    "Record 121-240: ts = 1693526401  \n",
    "Record 241-360: ts = 1693526402  \n",
    "….  \n",
    "1.3 Send your batch to a Kafka topic with an appropriate name.  \n",
    "\n",
    "All the data except for the ‘ts’ column should be sent in the original String type, without changing to any other types.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import sleep, time\n",
    "from json import dumps\n",
    "from kafka3 import KafkaProducer\n",
    "import random\n",
    "import datetime as dt\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "\n",
    "# configuration\n",
    "hostip = \"118.139.10.179\"\n",
    "topic = \"big-data-a2-topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        producer_instance.send(topic_name, data)\n",
    "#         print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode('ascii'),\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(file_path, max_rows=1000):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    return random.randint(0, file_size - max_rows * average_row_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_csv_fuc(file_path, min_rows=500, max_rows=1000):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    position = random.randint(0, file_size)\n",
    "    \n",
    "    num_rows = random.randint(min_rows, max_rows)\n",
    "    rows = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        f.seek(position)\n",
    "        f.readline() \n",
    "        reader = csv.reader(f)\n",
    "        for _ in range(num_rows):\n",
    "            line = next(reader, None)\n",
    "            if not line:\n",
    "                f.seek(0)\n",
    "                reader = csv.reader(f)\n",
    "                line = next(reader)\n",
    "            rows.append(line)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ts(rows):\n",
    "    ts = int(time())\n",
    "    batch = math.ceil(len(rows) / 5)\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        ts_increment = i // batch\n",
    "        row.append(ts + ts_increment)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Publishing records..')\n",
    "    producer = connect_kafka_producer()    \n",
    "    file_path = \"./dataset/click_stream_rt.csv\"\n",
    "    \n",
    "    while True:\n",
    "        rows = read_csv_fuc(file_path)\n",
    "        rows_ts = add_ts(rows)\n",
    "        \n",
    "        for row in rows_ts:\n",
    "            print(row)\n",
    "            publish_message(producer, topic, row)\n",
    "        \n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
