{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write code to create a SparkSession, which uses four cores with a proper application name, use the Melbourne timezone, and make sure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    clear_output(wait=True) \n",
    "    df.show(10, False) \n",
    "\n",
    "    \n",
    "topic = \"big-data-a2-topic\"\n",
    "host_ip = \"118.139.10.179\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0 pyspark-shell'\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, regexp_extract\n",
    "\n",
    "master = \"local[4]\"\n",
    "app_name = \"spark streaming\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"Australia/Melbourne\")\n",
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.streaming.stateStore.stateSchemaCheck\", \"false\")\n",
    "\n",
    "\n",
    "\n",
    "# Retrieve Spark context and set its log level to 'ERROR'.\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for customer:\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_version: string (nullable = true)\n",
      " |-- home_location_lat: float (nullable = true)\n",
      " |-- home_location_long: float (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_country: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, TimestampType\n",
    "\n",
    "# 1. Customer.csv Schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"#\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birthdate\", DateType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_version\", StringType(), True),\n",
    "    StructField(\"home_location_lat\", FloatType(), True),\n",
    "    StructField(\"home_location_long\", FloatType(), True),\n",
    "    StructField(\"home_location\", StringType(), True),\n",
    "    StructField(\"home_country\", StringType(), True),\n",
    "    StructField(\"first_join_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Load CSV files with predefined schemas\n",
    "directory_path = \"./dataset\"\n",
    "file_schemas = {\n",
    "    \"customer\": customer_schema,\n",
    "}\n",
    "\n",
    "for file_name, schema in file_schemas.items():\n",
    "    file_path = os.path.join(directory_path, file_name + \".csv\")\n",
    "    df_name = file_name + \"_df\"\n",
    "    globals()[df_name] = spark.read.format('csv')\\\n",
    "        .option('header', True).option('escape', '\"')\\\n",
    "        .schema(schema)\\\n",
    "        .load(file_path)\n",
    "    # drop redundant info\n",
    "    globals()[df_name] = globals()[df_name].drop(\"#\")\n",
    "    \n",
    "    # show schema\n",
    "    print(f\"Schema for {file_name}:\")\n",
    "    globals()[df_name].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Using the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt_stream =  spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "rt_stream.printSchema()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# main schema\n",
    "main_schema = StructType([    \n",
    "    StructField('session_id', StringType(), True), \n",
    "    StructField('event_name', StringType(), True),\n",
    "    StructField('event_id', StringType(), True),     \n",
    "    StructField('traffic_source', StringType(), True),     \n",
    "    StructField('customer_id', StringType(), True),\n",
    "    StructField('ts', TimestampType(), True),\n",
    "])\n",
    "\n",
    "# branch schema\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"item_price\", IntegerType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True),\n",
    "    StructField(\"search_keywords\", StringType(), True),\n",
    "    StructField(\"promo_code\", StringType(), True),\n",
    "    StructField(\"promo_amount\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get value of the kafka message\n",
    "stream_df = rt_stream.selectExpr(\"CAST(value AS STRING) AS value\", \"CAST(timestamp as timestamp)\")\n",
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Then, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.  \n",
    "Perform the following tasks:  \n",
    "a) For the 'ts' column, convert it to the timestamp format, we will use it as event_time.  \n",
    "b) If the data is late for more than 1 minute, discard it.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, trim, regexp_extract, regexp_replace, col, from_json\n",
    "\n",
    "cleaned_value = regexp_replace(stream_df[\"value\"], \"^\\[|\\]$\", \"\")\n",
    "split_col = split(cleaned_value, \",\")\n",
    "\n",
    "stream_df = stream_df.withColumn(\"event_metadata\", regexp_extract(cleaned_value, \"\\{.*\\}\", 0))\n",
    "\n",
    "stream_df = stream_df.select(\n",
    "    trim(regexp_replace(split_col.getItem(1), \"['\\\"]\", \"\")).alias('session_id'),\n",
    "    trim(regexp_replace(split_col.getItem(2), \"['\\\"]\", \"\")).alias('event_name'),\n",
    "    trim(regexp_replace(split_col.getItem(3), \"['\\\"]\", \"\")).alias('event_id'),\n",
    "    trim(regexp_replace(split_col.getItem(4), \"['\\\"]\", \"\")).alias('traffic_source'),\n",
    "    \"event_metadata\",\n",
    "    trim(regexp_replace(split_col.getItem(6), \"['\\\"]\", \"\")).alias('customer_id'),\n",
    "    split_col.getItem(7).cast(\"int\").alias('ts'),\n",
    "    \"timestamp\"\n",
    ")\n",
    "\n",
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, current_timestamp, expr, minute, col, unix_timestamp, current_timestamp\n",
    "\n",
    "# ts to timestamp\n",
    "stream_df = stream_df.withColumn(\"ts\", from_unixtime(col(\"ts\")).cast(\"timestamp\"))\n",
    "\n",
    "\n",
    "# Keep only rows from the last 1 minute\n",
    "stream_df = stream_df.where(\n",
    "    (unix_timestamp(current_timestamp()) - unix_timestamp(col(\"timestamp\"))) <= 60\n",
    ")\n",
    "\n",
    "# watermark\n",
    "stream_df = stream_df.withWatermark(\"timestamp\", \"1 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5  \n",
    "Aggregate the streaming data frame by session id and create features you used in your assignment 2A model. (note: customer ID has already been included in the stream.)   \n",
    "Then, join the static data frames with the streaming data frame as our final data for prediction.  \n",
    "Perform data type/column conversion according to your ML model, and print out the Schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, current_timestamp\n",
    "from pyspark.sql.functions import sum, count, when, col, first, collect_list\n",
    "\n",
    "category_1 = [\"ADD_PROMO\", \"ADD_TO_CART\"]\n",
    "category_2 = [\"VIEW_PROMO\", \"VIEW_ITEM\", \"SEARCH\"]\n",
    "category_3 = [\"SCROLL\", \"HOMEPAGE\", \"CLICK\"]\n",
    "\n",
    "agg_df = stream_df.withColumn(\n",
    "    \"is_promotion\",\n",
    "    when(col(\"event_name\") == \"ADD_PROMO\", 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"event_category\",\n",
    "    when(col(\"event_name\").isin(category_1), \"num_cat_highvalue\")\n",
    "    .when(col(\"event_name\").isin(category_2), \"num_cat_midvalue\")\n",
    "    .otherwise(\"num_cat_lowvalue\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, explode, from_json, col, first, last, struct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "\n",
    "# # DataFrame 2: Event Metadata\n",
    "df_event_metadata = agg_df.groupBy(\"session_id\").agg(\n",
    "    collect_list(\"event_metadata\").alias(\"event_metadata_list\")\n",
    ")\n",
    "\n",
    "df_event_metadata = df_event_metadata.select(\n",
    "    \"session_id\",\n",
    "    explode(col(\"event_metadata_list\")).alias(\"single_event_metadata\")\n",
    ")\n",
    "\n",
    "df_event_metadata = df_event_metadata.withColumn(\"metadata\", from_json(\"single_event_metadata\", metadata_schema))\n",
    "\n",
    "\n",
    "df_event_metadata = df_event_metadata.select(\n",
    "    \"session_id\",\n",
    "    \"metadata.product_id\",\n",
    "    \"metadata.quantity\",\n",
    "    \"metadata.item_price\"\n",
    ").filter(col(\"metadata.product_id\").isNotNull())  # Filter out null product_ids\n",
    "\n",
    "\n",
    "df_event_metadata = df_event_metadata.groupBy(\"session_id\", \"product_id\").agg(\n",
    "    last(\"quantity\").alias(\"quantity\"),\n",
    "    last(\"item_price\").alias(\"item_price\")\n",
    ")\n",
    "\n",
    "df_event_metadata = df_event_metadata.dropDuplicates([\"session_id\", \"product_id\"])\n",
    "\n",
    "\n",
    "df_event_metadata = df_event_metadata.groupBy(\"session_id\").agg(\n",
    "    collect_list(struct(\"product_id\", \"quantity\", \"item_price\")).alias(\"event_metadata_list\")\n",
    ")\n",
    "\n",
    "query_event_metadata = df_event_metadata.writeStream.outputMode(\"complete\") \\\n",
    "        .format(\"memory\")\\\n",
    "        .queryName(\"event_metadata_table\")\\\n",
    "        .trigger(processingTime='5 seconds')\\\n",
    "        .start()\n",
    "\n",
    "event_metadata_df = spark.sql(\"select * from event_metadata_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_event_metadata.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- num_cat_lowvalue: long (nullable = true)\n",
      " |-- is_promotion: integer (nullable = false)\n",
      " |-- event_metadata_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- item_price: integer (nullable = true)\n",
      " |-- high_value_ratio: double (nullable = true)\n",
      " |-- low_value_ratio: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame 3: Aggregated Data\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "df_agg = agg_df.groupBy(\"session_id\",\n",
    "    window(\"timestamp\", \"1 minutes\")\n",
    "                       ).agg(\n",
    "    first(\"customer_id\").alias(\"customer_id\"),\n",
    "    first(\"ts\").alias(\"ts\"),\n",
    "    first(\"traffic_source\").alias(\"traffic_source\"),\n",
    "    first(\"timestamp\").alias(\"timestamp\"),\n",
    "    sum(when(col(\"event_category\") == \"num_cat_highvalue\", 1).otherwise(0)).alias(\"num_cat_highvalue\"),\n",
    "    sum(when(col(\"event_category\") == \"num_cat_midvalue\", 1).otherwise(0)).alias(\"num_cat_midvalue\"),\n",
    "    sum(when(col(\"event_category\") == \"num_cat_lowvalue\", 1).otherwise(0)).alias(\"num_cat_lowvalue\"),\n",
    "    (when(sum(col(\"is_promotion\")) > 0, 1).otherwise(0)).alias(\"is_promotion\"),\n",
    "    count(\"*\").alias(\"total_actions\")\n",
    ")\n",
    "\n",
    "agg_df = df_agg.join(event_metadata_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "agg_df = agg_df.withColumn(\"high_value_ratio\", \n",
    "                                         (col(\"num_cat_highvalue\") / col(\"total_actions\")) * 100)\n",
    "agg_df = agg_df.withColumn(\"low_value_ratio\", \n",
    "                                         (col(\"num_cat_lowvalue\") / col(\"total_actions\")) * 100)\n",
    "\n",
    "agg_df = agg_df.drop(\"total_actions\")\n",
    "\n",
    "agg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_agg = agg_df.writeStream.outputMode(\"append\")\\\n",
    "#         .foreachBatch(foreach_batch_function)\\\n",
    "#         .trigger(processingTime='10 seconds')\\\n",
    "#         .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_agg.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = true)\n",
      " |-- num_cat_midvalue: long (nullable = true)\n",
      " |-- num_cat_lowvalue: long (nullable = true)\n",
      " |-- is_promotion: integer (nullable = false)\n",
      " |-- event_metadata_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- item_price: integer (nullable = true)\n",
      " |-- high_value_ratio: double (nullable = true)\n",
      " |-- low_value_ratio: double (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_version: string (nullable = true)\n",
      " |-- home_location_lat: float (nullable = true)\n",
      " |-- home_location_long: float (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_country: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- season: string (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, datediff, current_date, broadcast\n",
    "\n",
    "\n",
    "join_df = agg_df.join(broadcast(customer_df), on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "\n",
    "join_df = join_df.withColumn(\"month\", month(\"ts\"))\n",
    "\n",
    "join_df = join_df.withColumn(\"season\", \n",
    "    when(col(\"month\").between(3, 5), \"Spring\")\n",
    "    .when(col(\"month\").between(6, 8), \"Summer\")\n",
    "    .when(col(\"month\").between(9, 11), \"Autumn\")\n",
    "    .otherwise(\"Winter\")\n",
    ")\n",
    "\n",
    "join_df = join_df.withColumn(\"age\", (datediff(current_date(), col(\"birthdate\"))/365).cast('int'))\n",
    "join_df = join_df.withColumn(\"first_join_year\", year(\"first_join_date\"))\n",
    "\n",
    "join_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Load your ML model, and use the model to predict if each session will purchase according to the requirements below:\n",
    "a) Every 10 seconds, show the total number of potential sales transactions (prediction = 1) in the last 1 minute.   \n",
    "b) Every 30 seconds, show the total potential revenue in the last 30 seconds. “Potiential revenue” here is definded as: When prediction=1, extract customer shopping cart detail from metadata (sum of all items of ADD_TO_CART events).  \n",
    "c) Every 1 minute, show the top 10 best-selling products by total quantity. (note: No historical data is required, only the top 10 in each 1 minute window.)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- season: string (nullable = false)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_country: string (nullable = true)\n",
      " |-- home_location_lat: float (nullable = true)\n",
      " |-- home_location_long: float (nullable = true)\n",
      " |-- event_metadata_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- item_price: integer (nullable = true)\n",
      " |-- high_value_ratio: double (nullable = true)\n",
      " |-- low_value_ratio: double (nullable = true)\n",
      " |-- is_promotion: integer (nullable = false)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_select = [\"traffic_source\", \"season\", \"gender\", \"device_type\", \"home_location\", \"home_country\",\\\n",
    "                     \"home_location_lat\", \"home_location_long\", \"event_metadata_list\", \"high_value_ratio\",\\\n",
    "                     \"low_value_ratio\", \"is_promotion\", \"month\", \"age\", \"first_join_year\", \"timestamp\", \"customer_id\"]\n",
    "transformed_df = join_df.select(*columns_to_select)\n",
    "transformed_df = transformed_df.dropna(how=\"any\")\n",
    "\n",
    "# Displaying the schema\n",
    "transformed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_trans = transformed_df.writeStream.outputMode(\"append\")\\\n",
    "#         .foreachBatch(foreach_batch_function)\\\n",
    "#         .trigger(processingTime='1 seconds')\\\n",
    "#         .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_trans.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer columns\n",
    "categoryInputCols = [\"traffic_source\", \"season\", \"gender\", \"device_type\", \"home_location\"]\n",
    "numericInputCols = [\"high_value_ratio\", \"low_value_ratio\", \"is_promotion\", \"month\", \"age\",\n",
    "                    \"first_join_year\"]\n",
    "\n",
    "outputCols = [f'{x}_idx' for x in categoryInputCols]\n",
    "\n",
    "# StringIndexer \n",
    "inputIndexer = StringIndexer(inputCols=categoryInputCols, outputCols=outputCols, handleInvalid=\"keep\")\n",
    "\n",
    "# OneHotEncoder columns\n",
    "inputCols_OHE = [x for x in outputCols]\n",
    "outputCols_OHE = [f'{x}_vec' for x in categoryInputCols]\n",
    "\n",
    "# OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=inputCols_OHE, outputCols=outputCols_OHE)\n",
    "\n",
    "# Assembler columns\n",
    "assemblerInputs = outputCols_OHE + numericInputCols\n",
    "\n",
    "# Assembler\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\", handleInvalid = \"keep\")    \n",
    "\n",
    "# Gradient Boosted Tree Classifier\n",
    "gbt = GBTClassifier(labelCol=\"made_purchase\", featuresCol=\"features\")\n",
    "\n",
    "# pipeline for Gradient Boosted Tree\n",
    "pipeline_gbt = Pipeline(stages=[inputIndexer, encoder, assembler, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- season: string (nullable = false)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_country: string (nullable = true)\n",
      " |-- home_location_lat: float (nullable = true)\n",
      " |-- home_location_long: float (nullable = true)\n",
      " |-- event_metadata_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- product_id: integer (nullable = true)\n",
      " |    |    |-- quantity: integer (nullable = true)\n",
      " |    |    |-- item_price: integer (nullable = true)\n",
      " |-- high_value_ratio: double (nullable = true)\n",
      " |-- low_value_ratio: double (nullable = true)\n",
      " |-- is_promotion: integer (nullable = false)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- traffic_source_idx: double (nullable = false)\n",
      " |-- season_idx: double (nullable = false)\n",
      " |-- gender_idx: double (nullable = false)\n",
      " |-- device_type_idx: double (nullable = false)\n",
      " |-- home_location_idx: double (nullable = false)\n",
      " |-- traffic_source_vec: vector (nullable = true)\n",
      " |-- season_vec: vector (nullable = true)\n",
      " |-- gender_vec: vector (nullable = true)\n",
      " |-- device_type_vec: vector (nullable = true)\n",
      " |-- home_location_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6a\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "model = PipelineModel.load(\"./model_gbt/\")\n",
    "predictions_df = model.transform(transformed_df)\n",
    "predictions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_predictions = predictions_df.writeStream.outputMode(\"append\")\\\n",
    "#         .foreachBatch(\"console\")\\\n",
    "#         .trigger(processingTime='1 seconds')\\\n",
    "#         .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_predictions.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, sum, col\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "# 6a) Every 10 seconds, show the total number of potential sales transactions (prediction = 1) in the last 1 minute\n",
    "\n",
    "import uuid\n",
    "\n",
    "base_checkpoint_path = \"./checkpoints/\"\n",
    "count_checkpoint_path = base_checkpoint_path + str(uuid.uuid4())\n",
    "\n",
    "filter_df = predictions_df.filter(col(\"prediction\") == 1.0)\n",
    "\n",
    "windowedCounts = filter_df \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(window(col(\"timestamp\"), \"1 minute\", \"10 seconds\")) \\\n",
    "    .count()\n",
    "\n",
    "query_prediction = windowedCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"checkpointLocation\", count_checkpoint_path) \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query_prediction.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b\n",
    "# b) Every 30 seconds, show the total potential revenue in the last 30 seconds.\n",
    "\n",
    "from pyspark.sql.functions import explode, sum, window\n",
    "\n",
    "revenue_checkpoint_path = base_checkpoint_path + str(uuid.uuid4())\n",
    "\n",
    "\n",
    "# Explode the event_meta_list to extract each product\n",
    "exploded_df = filter_df.withColumn(\"event_meta_item\", explode(col(\"event_metadata_list\")))\n",
    "\n",
    "# Calculate potential revenue for each product\n",
    "revenue_df = exploded_df.withColumn(\"revenue\", col(\"event_meta_item.quantity\") * col(\"event_meta_item.item_price\"))\n",
    "\n",
    "# Calculate total potential revenue in the last 30 seconds using window function\n",
    "windowedRevenue = revenue_df \\\n",
    "    .withWatermark(\"timestamp\", \"30 seconds\") \\\n",
    "    .groupBy(window(col(\"timestamp\"), \"30 seconds\")) \\\n",
    "    .sum(\"revenue\")\\\n",
    "    .withColumnRenamed(\"sum(revenue)\", \"revenue\") \\\n",
    "\n",
    "# Stream the results\n",
    "query_revenue = windowedRevenue.writeStream.outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"checkpointLocation\", revenue_checkpoint_path) \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_revenue.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6c) Every 1 minute, show the top 10 best-selling products by total quantity.\n",
    "# from pyspark.sql.functions import explode, sum, window, col, desc\n",
    "\n",
    "# # 1. Group by product_id and window of 1 minute sliding every 10 seconds\n",
    "# product_agg = exploded_df.withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "#                          .groupBy(\"event_meta_item.product_id\", \n",
    "#                                   window(col(\"timestamp\"), \"1 minute\", \"10 seconds\")\n",
    "#                                  ).agg(\n",
    "#     sum(\"event_meta_item.quantity\").alias(\"total_quantity\")\n",
    "# )\n",
    "\n",
    "# # 2. Stream the results using console output\n",
    "# query_product = product_agg.writeStream.outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"truncate\", \"false\") \\\n",
    "#     .trigger(processingTime='10 seconds') \\\n",
    "#     .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6c) Every 1 minute, show the top 10 best-selling products by total quantity.\n",
    "\n",
    "product_checkpoint_path = base_checkpoint_path + str(uuid.uuid4())\n",
    "\n",
    "def process_order(df, epoch_id):\n",
    "    df = df.orderBy(desc(\"total_quantity\")).limit(10)\n",
    "    df.show()\n",
    "\n",
    "# 1. Group by product_id and window of 1 minute sliding every 10 seconds\n",
    "product_agg = exploded_df.withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "                         .groupBy(\"event_meta_item.product_id\", \n",
    "                                  window(col(\"timestamp\"), \"1 minute\", \"10 seconds\")\n",
    "                                 ).agg(\n",
    "    sum(\"event_meta_item.quantity\").alias(\"total_quantity\")\n",
    ")\n",
    "\n",
    "# 2. Stream the results using foreachBatch\n",
    "query_product = product_agg.writeStream.outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"checkpointLocation\", product_checkpoint_path) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_product.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7  \n",
    "a) Persist the prediction result along with cart metadata in parquet format; after that, read the parquet file and show the results to verify it is saved properly.  \n",
    "b) Persist the 30-second sales prediction in another parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+-----------+\n",
      "|prediction|event_metadata_list|timestamp|customer_id|\n",
      "+----------+-------------------+---------+-----------+\n",
      "+----------+-------------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7a\n",
    "from time import sleep\n",
    "import shutil\n",
    "\n",
    "\n",
    "selected_df = filter_df.withWatermark(\"timestamp\", \"1 minutes\")\\\n",
    "            .select(\"prediction\", \"event_metadata_list\", \"timestamp\", \"customer_id\")\n",
    "\n",
    "output_path = \"./output/predictions.parquet\"\n",
    "shutil.rmtree(output_path, ignore_errors=True)\n",
    "    \n",
    "def write_to_parquet(batch_df, batch_id):\n",
    "    batch_df.write.parquet(output_path, mode=\"append\")\n",
    "\n",
    "query_sink = selected_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(write_to_parquet) \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to process some data\n",
    "query_sink.awaitTermination(timeout=60)  # Wait for 60 seconds or until the query is terminated\n",
    "\n",
    "# Now, read the Parquet file\n",
    "prediction_sink = spark.read.load(output_path)\n",
    "prediction_sink.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------+\n",
      "|prediction| event_metadata_list|           timestamp|customer_id|\n",
      "+----------+--------------------+--------------------+-----------+\n",
      "|       1.0|[{39214, 1, 15964...|2023-10-19 10:20:...|      55963|\n",
      "|       1.0|[{20705, 1, 27147...|2023-10-19 10:20:...|       8997|\n",
      "|       1.0|[{19130, 4, 18968...|2023-10-19 10:20:...|      66840|\n",
      "|       1.0|[{13287, 1, 16265...|2023-10-19 10:18:...|      72301|\n",
      "|       1.0|[{35078, 1, 32088...|2023-10-19 10:20:...|      48622|\n",
      "|       1.0|[{53941, 1, 30837...|2023-10-19 10:20:...|      23696|\n",
      "|       1.0|[{49605, 1, 25232...|2023-10-19 10:20:...|      51927|\n",
      "|       1.0|[{11841, 2, 38563...|2023-10-19 10:18:...|      60118|\n",
      "|       1.0|[{47681, 1, 51961...|2023-10-19 10:20:...|      39406|\n",
      "|       1.0|[{47350, 2, 17022...|2023-10-19 10:20:...|      95334|\n",
      "|       1.0|[{51938, 1, 14871...|2023-10-19 10:18:...|      58344|\n",
      "|       1.0|[{22984, 7, 24040...|2023-10-19 10:20:...|      48626|\n",
      "|       1.0|[{18798, 1, 22212...|2023-10-19 10:20:...|       2968|\n",
      "|       1.0|[{45963, 1, 343330}]|2023-10-19 10:20:...|      47471|\n",
      "|       1.0|[{13348, 1, 25434...|2023-10-19 10:20:...|      54557|\n",
      "|       1.0|[{48022, 1, 175865}]|2023-10-19 10:20:...|      52644|\n",
      "|       1.0|[{19026, 1, 14924...|2023-10-19 10:20:...|      54233|\n",
      "|       1.0|[{51752, 1, 33670...|2023-10-19 10:20:...|      14683|\n",
      "|       1.0|[{54502, 1, 63075...|2023-10-19 10:20:...|      29360|\n",
      "|       1.0|[{4573, 10, 471196}]|2023-10-19 10:20:...|      87353|\n",
      "+----------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_sink = spark.read.load(output_path)\n",
    "prediction_sink.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7b\n",
    "# Save to Parquet\n",
    "import shutil\n",
    "\n",
    "output_path_r = \"./output/revenue.parquet\"\n",
    "shutil.rmtree(output_path_r, ignore_errors=True)\n",
    "    \n",
    "def write_new_parquet(batch_df, batch_id):\n",
    "    batch_df.write.parquet(output_path_r, mode=\"append\")\n",
    "#     batch_df.write.mode(\"append\").save(output_path_r)\n",
    "\n",
    "query_sink_r = windowedRevenue.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(write_new_parquet) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query_sink_r.awaitTermination(timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_sink_r.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|              window| revenue|\n",
      "+--------------------+--------+\n",
      "|{2023-10-19 10:20...|10662908|\n",
      "|{2023-10-19 10:20...|19669485|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_sink = spark.read.load(output_path_r)\n",
    "sale_sink.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8  \n",
    "Read the parquet files as a data stream, for 7a) join customer information and send to a Kafka topic with an appropriate name to the data visualisation. For 7b) Send the message directly to another Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream 1\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "static_df = spark.read.parquet(output_path)\n",
    "schema = static_df.schema\n",
    "\n",
    "parquet_stream = spark.readStream.schema(schema).parquet(output_path)\n",
    "parquet_stream = parquet_stream.join(broadcast(customer_df), on=\"customer_id\", how=\"inner\")\n",
    "parquet_stream = parquet_stream.selectExpr(\"to_json(struct(*)) AS value\")\n",
    "parquet_stream.printSchema()\n",
    "\n",
    "query_send = parquet_stream.writeStream \\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\") \\\n",
    "    .option(\"topic\", \"for_visualisation\") \\\n",
    "    .option(\"checkpointLocation\", \"./output/check/for_visualisation\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# parquet_stream = spark.readStream.schema(schema).parquet(output_path)\n",
    "# parquet_stream = parquet_stream.withWatermark(\"timestamp\", \"30 seconds\")\n",
    "# parquet_stream = parquet_stream.alias(\"stream\").join(broadcast(customer_df), on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# current_time = F.current_timestamp()\n",
    "# five_minutes_ago = current_time - F.expr(\"INTERVAL 30 SECONDS\")\n",
    "# filtered_stream = parquet_stream.filter((parquet_stream[\"stream.timestamp\"] >= five_minutes_ago) & (parquet_stream[\"stream.timestamp\"] <= current_time))\n",
    "\n",
    "# filtered_stream = filtered_stream.selectExpr(\"to_json(struct(*)) AS value\")\n",
    "\n",
    "# query_send = filtered_stream.writeStream \\\n",
    "#     .outputMode(\"append\")\\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\") \\\n",
    "#     .option(\"topic\", \"for_visualisation\") \\\n",
    "#     .option(\"checkpointLocation\", \"./output/check/for_visualisation\") \\\n",
    "#     .trigger(processingTime='30 seconds') \\\n",
    "#     .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_send.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_send.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream 2\n",
    "static_df_2 = spark.read.parquet(output_path_r)\n",
    "schema_2 = static_df_2.schema\n",
    "\n",
    "parquet_stream_2 = spark.readStream.schema(schema_2).parquet(output_path_r)\n",
    "parquet_stream_2 = parquet_stream_2.selectExpr(\"to_json(struct(*)) AS value\")\n",
    "parquet_stream_2.printSchema()\n",
    "\n",
    "query_topic = parquet_stream_2.writeStream \\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\") \\\n",
    "    .option(\"topic\", \"another_topic\") \\\n",
    "    .option(\"checkpointLocation\", \"./output/check/another_topic\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# static_df_2 = spark.read.parquet(output_path_r)\n",
    "# schema_2 = static_df_2.schema\n",
    "\n",
    "\n",
    "# parquet_stream_2 = spark.readStream.schema(schema_2).parquet(output_path_r)\n",
    "# parquet_stream_2 = parquet_stream_2.withColumn(\"start_time\", parquet_stream_2.window.start)\n",
    "# parquet_stream_2 = parquet_stream_2.withWatermark(\"start_time\", \"30 seconds\")\n",
    "\n",
    "# current_time = F.current_timestamp()\n",
    "# thirty_seconds_ago = current_time - F.expr(\"INTERVAL 30 SECONDS\")\n",
    "# filtered_stream_2 = parquet_stream_2.filter((parquet_stream_2[\"start_time\"] >= thirty_seconds_ago) & (parquet_stream_2[\"start_time\"] <= current_time))\n",
    "\n",
    "# filtered_stream_2 = filtered_stream_2.selectExpr(\"to_json(struct(*)) AS value\")\n",
    "\n",
    "# query_topic = filtered_stream_2.writeStream \\\n",
    "#     .outputMode(\"append\")\\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\") \\\n",
    "#     .option(\"topic\", \"another_topic\") \\\n",
    "#     .option(\"checkpointLocation\", \"./output/check/another_topic\") \\\n",
    "#     .trigger(processingTime='30 seconds') \\\n",
    "#     .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_topic.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_topic.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stream 3\n",
    "# query_skip = filter_df.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\")\\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\") \\\n",
    "#     .option(\"topic\", \"skip\") \\\n",
    "#     .option(\"checkpointLocation\", \"./output/skip_new\")\\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_skip.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_skip.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
